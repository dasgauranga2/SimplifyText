{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from transformers import T5Model,T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hello', '▁world', '▁how', '▁are', '▁you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello world how are you?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8774, 296, 149, 33, 25, 58]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> </s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.pad_token\n",
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 2\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['t5-small']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)\n",
    "\n",
    "TRG = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = data.TabularDataset.splits(\n",
    "                path = '',\n",
    "                train = 'news_train.csv',\n",
    "                format = 'csv',\n",
    "                fields = fields,\n",
    "                skip_header = True)\n",
    "\n",
    "train_data , valid_data = train_data[0].split(split_ratio=0.98,\n",
    "                                             random_state = random.seed(4321))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86765\n",
      "1771\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.examples))\n",
    "print(len(valid_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': [3, 76, 17, 2046, 3, 5319, 1395, 107, 21108, 5752, 6323, 176, 15, 7, 107, 3, 7, 17178, 9, 30, 3, 189, 3589, 1135, 243, 24, 1181, 18, 11956, 32, 12025, 16, 8, 538, 65, 118, 3759, 12, 766, 8, 1455, 13, 887, 11, 59, 12, 31738, 16679, 151, 5, 96, 29, 32, 6965, 56, 36, 3169, 26, 406, 136, 1053, 5, 62, 33, 464, 21, 8, 394, 297, 13, 151, 5, 3, 99, 10843, 56, 6136, 120, 31738, 13112, 6, 62, 56, 59, 8179, 135, 976, 3, 88, 974, 5], 'trg': [1181, 18, 11956, 32, 12025, 59, 3759, 12, 31738, 16679, 7, 10, 95, 3, 26, 63, 2446]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[4000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'u', 't', 'tar', '▁', 'pra', 'des', 'h', '▁deputy', '▁chief', '▁minister', '▁din', 'e', 's', 'h', '▁', 's', 'harm', 'a', '▁on', '▁', 'th', 'urs', 'day', '▁said', '▁that', '▁anti', '-', 'rome', 'o', '▁squad', '▁in', '▁the', '▁state', '▁has', '▁been', '▁launched', '▁to', '▁ensure', '▁the', '▁safety', '▁of', '▁women', '▁and', '▁not', '▁to', '▁disturb', '▁innocent', '▁people', '.', '▁\"', 'n', 'o', 'body', '▁will', '▁be', '▁trouble', 'd', '▁without', '▁any', '▁reason', '.', '▁we', '▁are', '▁working', '▁for', '▁the', '▁better', 'ment', '▁of', '▁people', '.', '▁', 'if', '▁somebody', '▁will', '▁false', 'ly', '▁disturb', '▁anybody', ',', '▁we', '▁will', '▁not', '▁spare', '▁them', ',\"', '▁', 'he', '▁added', '.']\n",
      "['▁anti', '-', 'rome', 'o', '▁squad', '▁not', '▁launched', '▁to', '▁disturb', '▁innocent', 's', ':', '▁up', '▁', 'd', 'y', '▁cm']\n"
     ]
    }
   ],
   "source": [
    "src_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[4000])['src'])\n",
    "trg_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[4000])['trg'])\n",
    "\n",
    "print(src_tokens)\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "                                 (train_data, valid_data), \n",
    "                                 batch_size = BATCH_SIZE,\n",
    "                                 device = device,\n",
    "                                 sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.t5 = T5Model.from_pretrained('t5-small')\n",
    "        \n",
    "        self.out = nn.Linear(self.t5.config.to_dict()['d_model'],\n",
    "                             self.t5.config.to_dict()['vocab_size'])\n",
    "                \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        embedded = self.t5(input_ids=src,decoder_input_ids=trg) \n",
    "        \n",
    "        output = self.out(embedded[0])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Network().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 76,988,544 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1\tTRAIN LOSS : 2.58\tVALID LOSS : 1.38\tTIME : 1269.99\n",
      "\n",
      "EPOCH : 2\tTRAIN LOSS : 1.31\tVALID LOSS : 1.25\tTIME : 1311.11\n",
      "\n",
      "EPOCH : 3\tTRAIN LOSS : 1.11\tVALID LOSS : 1.20\tTIME : 1782.60\n",
      "\n",
      "EPOCH : 4\tTRAIN LOSS : 0.99\tVALID LOSS : 1.21\tTIME : 1279.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = model.float()\n",
    "    # TRAINING\n",
    "    ##############################################################################\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    train_loss = epoch_loss / len(train_iterator)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # VALIDATION\n",
    "    ##############################################################################\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    valid_loss = epoch_loss / len(valid_iterator)\n",
    "    ##############################################################################\n",
    "    model = model.half()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"EPOCH : {epoch+1}\\tTRAIN LOSS : {train_loss:.2f}\\tVALID LOSS : {valid_loss:.2f}\\tTIME : {end_time-start_time:.2f}\\n\")\n",
    "    torch.save(model.state_dict(), f't5_summ_model_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONVERT ALL MODEL WEIGHTS AND BIASES TO HALF PRECISION\n",
    "# # MODEL SIZE WILL REDUCE\n",
    "# model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 't5_summ_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence2(sentence, eval_model, device, max_len = 50):\n",
    "    \n",
    "    eval_model.eval()\n",
    "\n",
    "    src_indexes = [init_token_idx] + sentence + [eos_token_idx]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_indexes = [init_token_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = eval_model(src_tensor, trg_tensor)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == eos_token_idx:\n",
    "            break\n",
    "\n",
    "    return trg_indexes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC : ▁more▁than▁100▁state▁and▁district▁roads▁in▁uttar▁pradesh▁will▁be▁converted▁into▁national▁highways,▁deputy▁chief▁minister▁keshav▁prasad▁maurya▁said▁on▁sunday.▁maurya▁further▁claimed▁that▁the▁bjp▁has▁already▁succeeded▁in▁making▁a▁major▁part▁of▁the▁state's▁roads▁free▁of▁potholes▁and▁that▁the▁city▁of▁allahabad▁would▁witness▁rapid▁development▁in▁near▁future.n\n",
      "PRED : ▁100▁up▁state,▁district▁roads▁to▁be▁converted▁into▁highways</s>\n",
      "\n",
      "SRC : ▁harvard▁and▁cornell▁university▁researchers▁have▁documented▁the▁\"dance▁routine\"▁of▁vogelkop▁superb▁bird-of-paradise,▁which▁helped▁confirm▁it▁as▁a▁new▁species.▁the▁video▁revealed▁the▁male▁dances▁with▁crescent-shaped▁wings▁to▁lure▁its▁female▁counterpart.▁the▁species▁was▁earlier▁confused▁with▁a▁similar▁looking▁bird▁which▁is▁also▁endemic▁to▁the▁island▁nation▁of▁new▁guinea,▁off▁australia's▁coast.\n",
      "PRED : ▁bird-of-paradise▁dances▁with▁crescent▁wings▁to▁lure▁its▁female</s>\n",
      "\n",
      "SRC : ▁two▁security▁personnel▁have▁committed▁suicide▁in▁jaisalmer▁within▁a▁span▁of▁two▁days.▁a▁bsf▁head▁constable▁allegedly▁shot▁himself▁with▁his▁official▁weapon▁on▁tuesday▁as▁he▁was▁reportedly▁facing▁domestic▁issues.▁the▁incident▁occurred▁less▁than▁a▁day▁after▁38-year-old▁army▁jawan▁jogendra▁singh▁allegedly▁committed▁suicide▁by▁hanging▁himself▁inside▁a▁store.\n",
      "PRED : ▁2▁security▁personnel▁commit▁suicide▁in▁jaisalmer▁in▁2▁days</s>\n",
      "\n",
      "SRC : ▁brazilian▁football▁legend▁pel<unk>,▁whose▁full▁name▁is▁edson▁arantes▁do▁nascimento,▁was▁named▁after▁american▁inventor▁thomas▁alva▁edison▁as▁electricity▁had▁just▁been▁introduced▁to▁his▁hometown▁when▁he▁was▁born.▁pel<unk>,▁who▁turns▁78▁today,▁had▁stated▁that▁he▁was▁proud▁to▁be▁named▁after▁edison▁and▁initially▁hated▁the▁nickname▁pel<unk>▁as▁it▁sounded▁\"horrible\".\n",
      "PRED : ▁pel<unk>▁named▁after▁american▁inventor▁as▁electricity▁was▁introduced</s>\n",
      "\n",
      "SRC : ▁actor▁amitabh▁bachchan▁took▁to▁twitter▁to▁deny▁reports▁that▁he▁escaped▁a▁car▁accident▁in▁kolkata▁while▁travelling▁to▁the▁airport.▁\"that▁is▁incorrect...▁there▁has▁been▁no▁accident...▁i▁am▁well,\"▁he▁tweeted.▁the▁reports▁also▁said▁that▁the▁state▁government▁had▁issued▁a▁show▁cause▁notice▁to▁the▁travel▁agency▁which▁had▁provided▁the▁car.\n",
      "PRED : ▁big▁b▁denies▁reports▁of▁escaping▁car▁accident▁in▁kolkata</s>\n",
      "\n",
      "SRC : ▁amrita▁singh,▁while▁talking▁about▁her▁daughter▁sara▁ali▁khan,▁revealed,▁\"she▁would▁always▁do▁those▁pretty▁things▁in▁front▁of▁the▁mirror.▁we▁knew▁that▁she▁would▁definitely▁become▁an▁actress.\"▁amrita▁added▁that▁sara▁is▁a▁born▁actor.▁\"i▁hope▁sara▁does...well▁in▁life.▁i▁want▁her▁to▁stay▁extremely▁grounded,▁focussed...committed▁to▁her▁work,\"▁amrita▁further▁said.\n",
      "PRED : ▁sara▁would▁always▁do▁pretty▁things▁in▁front▁of▁mirror:▁amrita▁singh</s>\n",
      "\n",
      "SRC : ▁kannada▁television▁actress▁rekha▁sindhu▁was▁killed▁in▁a▁car▁accident▁early▁on▁friday▁morning▁near▁vellore▁in▁tamil▁nadu.▁the▁incident▁occurred▁on▁the▁chennai-bangalore▁highway.▁she▁was▁reportedly▁with▁three▁other▁people,▁who▁were▁also▁killed▁in▁the▁accident.▁the▁names▁of▁the▁other▁deceased▁have▁been▁reported▁as▁abhishek▁kumaran,▁jayankandran▁and▁rakshan.\n",
      "PRED : ▁kannada▁actress▁rekha▁sindhu▁killed▁in▁car▁accident</s>\n",
      "\n",
      "SRC : ▁the▁supreme▁court▁on▁tuesday▁said▁that▁further▁hearing▁on▁ayodhya▁dispute▁will▁be▁held▁on▁february▁8,▁2018.▁this▁comes▁a▁day▁ahead▁of▁the▁babri▁masjid▁demolition's▁25th▁anniversary.▁arguing▁that▁pleadings▁in▁the▁case▁aren't▁complete,▁lawyers▁representing▁the▁sunni▁waqf▁board▁requested▁the▁court▁to▁take▁up▁the▁matter▁in▁july▁2019▁after▁the▁general▁elections.\n",
      "PRED : ▁ayodhya▁dispute▁to▁be▁held▁on▁february▁8:▁sc</s>\n",
      "\n",
      "SRC : ▁switzerland-based▁startup▁sirin▁labs▁has▁unveiled▁the▁design▁for▁a▁$1,000▁dual-screen▁'secure'▁blockchain▁smartphone▁called▁finney▁which▁is▁endorsed▁by▁football▁player▁lionel▁messi.▁running▁sirin▁os,▁the▁phone▁comes▁with▁a▁built-in▁cold▁storage▁crypto▁wallet▁which▁supports▁major▁cryptocurrencies▁and▁tokens.▁it▁also▁features▁6▁gb▁of▁ram,▁128▁gb▁of▁storage,▁and▁a▁12-megapixel▁rear▁camera.\n",
      "PRED : ▁startup▁unveils▁design▁for▁$10,000▁'secure'▁blockchain▁phone</s>\n",
      "\n",
      "SRC : ▁amid▁allegations▁that▁rotomac▁pens'▁owner▁vikram▁kothari▁defrauded▁state-owned▁banks▁of▁more▁than▁â<unk>1800▁crore,▁his▁lawyer▁has▁said▁it▁is▁a▁case▁of▁loan▁default.▁the▁cbi▁has▁registered▁a▁case▁of▁wilful▁loan▁default▁involving▁vikram▁kothari▁and▁others▁based▁on▁a▁complaint▁by▁bank▁of▁baroda.▁following▁this,▁the▁cbi▁raided▁kothari's▁homes▁and▁offices.\n",
      "PRED : ▁rotomac▁pens▁owner▁accused▁of▁defrauding▁banks▁of▁â<unk>1800▁crore</s>\n",
      "\n",
      "SRC : ▁bengaluru▁development▁minister▁kj▁george▁has▁filed▁a▁criminal▁defamation▁suit▁against▁former▁times▁now▁editor-in-chief▁arnab▁goswami▁and▁the▁channel.▁the▁minister▁accused▁arnab▁for▁making▁statements▁that▁\"insinuate▁kj▁george▁having▁a▁vested▁interest▁in▁dk▁raviâ<unk>s▁death\"▁in▁march▁2015.▁notably,▁cbi▁had▁concluded▁in▁november▁2015▁that▁the▁cause▁of▁ias▁officer▁ravi's▁demise▁was▁suicide.\n",
      "PRED : ▁b'luru▁minister▁files▁criminal▁defamation▁suit▁against▁arnab▁goswami</s>\n",
      "\n",
      "SRC : ▁goa▁chief▁minister▁manohar▁parrikar▁will▁contest▁bypoll▁from▁state▁capital▁panaji,▁goa▁bjp▁chief▁vinay▁tendulkar▁said▁on▁wednesday.▁parrikar▁resigned▁as▁the▁defence▁minister▁to▁assume▁the▁chief▁ministerial▁office▁but▁is▁not▁a▁member▁of▁the▁40-member▁state▁legislative▁assembly.▁the▁61-year-old▁leader▁needs▁to▁get▁elected▁to▁the▁assembly▁within▁six▁months▁of▁taking▁office.\n",
      "PRED : ▁goa▁cm▁manohar▁parrikar▁to▁contest▁bypoll▁from▁panaji</s>\n",
      "\n",
      "SRC : ▁actor▁arjun▁kapoor▁said▁that▁films▁are▁made▁for▁audience▁and▁not▁for▁a▁limited▁bunch▁of▁people▁who▁call▁themselves▁critics.▁\"the▁audience▁is▁more▁open▁to▁giving▁the▁film▁space▁and▁time▁to▁enjoy▁what▁it▁is▁meant▁for▁rather▁than▁having▁preconceived▁notions,\"▁added▁arjun.▁he▁further▁said▁that▁audience▁is▁the▁most▁important▁part▁of▁filmmaking.\n",
      "PRED : ▁films▁made▁for▁audience,▁not▁critics:▁arjun▁kapoor</s>\n",
      "\n",
      "SRC : ▁an▁engineer▁from▁kota▁has▁been▁fighting▁for▁a▁year▁to▁get▁â<unk>135▁refund▁from▁irctc▁after▁the▁amount▁was▁deducted▁as▁service▁tax▁despite▁him▁cancelling▁the▁ticket▁before▁the▁gst▁implementation.▁according▁to▁rules,▁for▁any▁ticket▁booked▁in▁advance▁for▁the▁post-gst▁period▁and▁cancelled▁before▁gst▁implementation,▁the▁service▁tax▁amount▁had▁to▁be▁refunded.\n",
      "PRED : ▁man▁fights▁for▁â<unk>135▁refund▁after▁gst▁ticket▁cancellation</s>\n",
      "\n",
      "SRC : ▁law▁student▁sanam▁setia▁was▁arrested▁in▁punjab's▁kharar▁for▁opening▁fire▁at▁domino's▁pizza▁staff▁after▁an▁argument▁on▁being▁delivered▁a▁cold▁pizza.▁setia▁abused▁the▁delivery▁boy▁and▁refused▁to▁pay,▁following▁which▁the▁outlet's▁manager▁and▁two▁staffers▁reached▁his▁house.▁setia▁then▁got▁his▁licensed▁gun▁and▁opened▁fire▁at▁them,▁though▁they▁escaped▁unhurt.\n",
      "PRED : ▁law▁student▁arrested▁for▁opening▁fire▁at▁domino's▁pizza▁staff</s>\n",
      "\n",
      "SRC : ▁scientists▁have▁discovered▁that▁mitochondria,▁known▁as▁the▁powerhouse▁of▁cells▁function▁at▁a▁temperature▁of▁50â°c,▁much▁hotter▁than▁body▁average▁of▁37â°c.▁mitochondria▁convert▁oxygen▁and▁nutrients▁into▁atp▁(cells'▁primary▁energy▁source).▁scientists▁used▁'fluorescent▁thermometers',▁temperature-sensitive▁dyes▁that▁bind▁with▁cell▁organelles▁to▁find▁out▁operating▁temperatures▁of▁mitochondrion▁enzymes.\n",
      "PRED : ▁scientists▁discover▁how▁cells▁work▁at▁50â°c</s>\n",
      "\n",
      "SRC : ▁the▁teaser▁of▁reality▁series▁'life▁of▁kylie',▁which▁will▁showcase▁the▁life▁of▁television▁personality▁kylie▁jenner▁has▁been▁released.▁it▁depicts▁jenner▁talking▁about▁being▁raised▁in▁front▁of▁the▁camera▁for▁'keeping▁up▁with▁the▁kardashians'.▁\"when▁you▁grow▁up▁on▁camera,▁everybody▁feels▁like▁they▁know▁you.▁but▁they▁don't,\"▁said▁kylie▁in▁the▁teaser.\n",
      "PRED : ▁teaser▁of▁'life▁of▁kylie'▁to▁showcase▁kylie▁jenner's▁life</s>\n",
      "\n",
      "SRC : ▁us▁printer▁and▁copier▁company▁xerox▁has▁called▁off▁a▁$6.1-billion▁takeover▁by▁japan's▁fujifilm▁holdings▁in▁a▁settlement▁with▁activist▁investors▁carl▁icahn▁and▁darwin▁deason.▁the▁two▁investors,▁who▁together▁own▁15%▁of▁xerox,▁had▁opposed▁the▁deal▁saying▁it▁undervalued▁the▁firm.nthe▁settlement▁will▁see▁ceo▁jeff▁jacobson,▁as▁well▁as▁five▁other▁directors,▁step▁down.\n",
      "PRED : ▁xerox▁calls▁off▁$6.1-billion▁takeover▁by▁fujifilm</s>\n",
      "\n",
      "SRC : ▁india's▁first▁mission▁to▁mars,▁mars▁orbiter▁mission,▁has▁successfully▁completed▁three▁years▁in▁orbit,▁despite▁being▁designed▁to▁last▁only▁six▁months.▁mangalyaan,▁which▁reached▁its▁martian▁orbit▁on▁september▁24,▁2014,▁using▁least▁amount▁of▁fuel▁possible,▁is▁the▁cheapest▁mars▁mission▁in▁history.▁it▁was▁launched▁utilising▁a▁unique▁earth-mars▁arrangement▁that▁occurs▁once▁in▁780▁days.\n",
      "PRED : ▁mars▁mission▁completes▁3▁years▁in▁orbit▁despite▁being▁designed▁to▁last▁6▁months</s>\n",
      "\n",
      "SRC : ▁liquor▁firm▁united▁spirits▁has▁more▁'crorepatis'▁than▁infosys,▁and▁all▁indian▁fmcg▁companies▁except▁hindustan▁unilever,▁reports▁said.▁around▁56▁executives▁at▁united▁spirits▁earned▁over▁â<unk>11▁crore▁for▁2016-17.▁an▁hr▁consultant▁said▁when▁vijay▁mallya▁was▁at▁the▁helm▁of▁united▁spirits,▁very▁few▁people▁were▁paid▁very▁high▁salaries,▁but▁now▁it's▁being▁run▁more▁professionally.\n",
      "PRED : ▁united▁spirits▁has▁more▁'crorepatis'▁than▁infosys:▁report</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0, len(valid_data.examples)), 20)\n",
    "for i in idxs:\n",
    "    src = vars(valid_data.examples[i])['src']\n",
    "    #trg = vars(valid_data.examples[i])['trg']\n",
    "    translation = translate_sentence2(src, model, device)\n",
    "\n",
    "    print(f\"SRC : {''.join(tokenizer.convert_ids_to_tokens(src))}\")\n",
    "    #print(f\"TRG : {''.join(tokenizer.convert_ids_to_tokens(trg))}\")\n",
    "    print(f\"PRED : {''.join(tokenizer.convert_ids_to_tokens(translation))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
