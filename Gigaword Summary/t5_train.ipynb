{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from transformers import T5Model,T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hello', '▁world', '▁how', '▁are', '▁you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello world how are you?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8774, 296, 149, 33, 25, 58]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> </s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.pad_token\n",
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 2\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['t5-small']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)\n",
    "\n",
    "TRG = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = data.TabularDataset.splits(\n",
    "                path = '',\n",
    "                train = 'gigaword_train.csv',\n",
    "                format = 'csv',\n",
    "                fields = fields,\n",
    "                skip_header = True)\n",
    "\n",
    "train_data , valid_data = train_data[0].split(split_ratio=0.98,\n",
    "                                             random_state = random.seed(4321))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88811\n",
      "1812\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.examples))\n",
    "print(len(valid_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': [23407, 3, 31, 7, 3, 1824, 288, 9, 7, 19184, 243, 3, 189, 3589, 1135, 34, 133, 13321, 3429, 49, 1034, 11642, 11, 27755, 7415, 21, 66, 7534, 3140, 45, 11, 11811, 53, 190, 3, 115, 10694, 77, 788, 12, 8, 3, 30714, 17187, 5685, 3, 5], 'trg': [23407, 3, 31, 7, 3, 1824, 288, 9, 7, 6456, 7, 6926, 49, 1034, 21, 3, 1598, 7534]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[4000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁australia', '▁', \"'\", 's', '▁', 'q', 'ant', 'a', 's', '▁airlines', '▁said', '▁', 'th', 'urs', 'day', '▁it', '▁would', '▁enforce', '▁tough', 'er', '▁security', '▁checks', '▁and', '▁baggage', '▁controls', '▁for', '▁all', '▁flights', '▁leaving', '▁from', '▁and', '▁transit', 'ing', '▁through', '▁', 'b', 'rita', 'in', '▁due', '▁to', '▁the', '▁', 'heightened', '▁terror', '▁alert', '▁', '.']\n",
      "['▁australia', '▁', \"'\", 's', '▁', 'q', 'ant', 'a', 's', '▁announce', 's', '▁strict', 'er', '▁security', '▁for', '▁', 'uk', '▁flights']\n"
     ]
    }
   ],
   "source": [
    "src_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[4000])['src'])\n",
    "trg_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[4000])['trg'])\n",
    "\n",
    "print(src_tokens)\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "                                 (train_data, valid_data), \n",
    "                                 batch_size = BATCH_SIZE,\n",
    "                                 device = device,\n",
    "                                 sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.t5 = T5Model.from_pretrained('t5-small')\n",
    "        \n",
    "        self.out = nn.Linear(self.t5.config.to_dict()['d_model'],\n",
    "                             self.t5.config.to_dict()['vocab_size'])\n",
    "                \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        embedded = self.t5(input_ids=src,decoder_input_ids=trg) \n",
    "        \n",
    "        output = self.out(embedded[0])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Network().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 76,988,544 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0004\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 4\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 1\tTRAIN LOSS : 2.87\tVALID LOSS : 1.80\tTIME : 938.04\n",
      "\n",
      "EPOCH : 2\tTRAIN LOSS : 1.68\tVALID LOSS : 1.64\tTIME : 966.25\n",
      "\n",
      "EPOCH : 3\tTRAIN LOSS : 1.45\tVALID LOSS : 1.58\tTIME : 968.45\n",
      "\n",
      "EPOCH : 4\tTRAIN LOSS : 1.31\tVALID LOSS : 1.56\tTIME : 969.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = model.float()\n",
    "    # TRAINING\n",
    "    ##############################################################################\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(train_iterator):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg[:,:-1])\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:,1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    train_loss = epoch_loss / len(train_iterator)\n",
    "    ##############################################################################\n",
    "    \n",
    "    # VALIDATION\n",
    "    ##############################################################################\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(valid_iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg[:,:-1])\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    valid_loss = epoch_loss / len(valid_iterator)\n",
    "    ##############################################################################\n",
    "    model = model.half()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"EPOCH : {epoch+1}\\tTRAIN LOSS : {train_loss:.2f}\\tVALID LOSS : {valid_loss:.2f}\\tTIME : {end_time-start_time:.2f}\\n\")\n",
    "    torch.save(model.state_dict(), f't5_summ_model_{epoch+1}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CONVERT ALL MODEL WEIGHTS AND BIASES TO HALF PRECISION\n",
    "# # MODEL SIZE WILL REDUCE\n",
    "# model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 't5_summ_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence2(sentence, eval_model, device, max_len = 50):\n",
    "    \n",
    "    eval_model.eval()\n",
    "\n",
    "    src_indexes = [init_token_idx] + sentence + [eos_token_idx]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_indexes = [init_token_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = eval_model(src_tensor, trg_tensor)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == eos_token_idx:\n",
    "            break\n",
    "\n",
    "    return trg_indexes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC : ▁french▁foreign▁minister▁herve▁de▁charette▁on▁friday▁gave▁serb▁president▁slobodan▁milosevic▁until▁next▁sunday▁to▁return▁two▁french▁pilots▁missing▁since▁their▁mirage▁jet▁was▁shot▁down▁over▁serb▁held▁territory▁in▁bosnia▁since▁august▁##▁.\n",
      "PRED : ▁france▁gives▁milosevic▁until▁sunday▁to▁return▁missing▁pilots</s>\n",
      "\n",
      "SRC : ▁defending▁champions▁paris▁st.▁germain▁closed▁to▁within▁four▁points▁of▁pacesetters▁nantes▁at▁the▁top▁of▁the▁french▁first▁division▁on▁friday▁when▁they▁snatched▁a▁#-#▁win▁in▁their▁away▁clash▁at▁lens▁.\n",
      "PRED : ▁paris▁st.▁germain▁close▁to▁nantes</s>\n",
      "\n",
      "SRC : ▁bosnian▁prime▁minister▁haris▁silajdzic▁said▁he▁would▁lead▁a▁government▁delegation▁to▁geneva▁monday▁for▁talks▁with▁the▁international▁contact▁group▁on▁the▁war▁in▁his▁country▁.\n",
      "PRED : ▁bosnian▁pm▁to▁head▁delegation▁to▁geneva</s>\n",
      "\n",
      "SRC : ▁chelsea▁defender▁celestine▁babayaro▁has▁been▁given▁up▁till▁may▁##▁to▁prove▁he▁is▁fit▁for▁the▁world▁cup▁,▁officials▁told▁afp▁tuesday▁.\n",
      "PRED : ▁babayaro▁given▁up▁till▁may▁##</s>\n",
      "\n",
      "SRC : ▁kevin▁dineen▁,▁one▁of▁just▁eight▁players▁in▁national▁hockey▁league▁history▁with▁###▁goals▁and▁#,###▁penalty▁minutes▁,▁announced▁his▁retirement▁here▁tuesday▁.\n",
      "PRED : ▁dineen▁retires▁from▁nhl</s>\n",
      "\n",
      "SRC : ▁us▁defense▁secretary▁robert▁gates▁on▁monday▁hailed▁the▁us▁leaders▁of▁the▁surge▁strategy▁on▁the▁eve▁of▁a▁military▁change▁of▁command▁in▁iraq▁,▁but▁said▁the▁new▁commanders▁will▁face▁<unk>▁a▁mission▁of▁transition▁''▁as▁american▁troop▁levels▁shrink▁.\n",
      "PRED : ▁gates▁hails▁us▁leaders▁of▁surge▁strategy▁on▁eve▁of▁iraq</s>\n",
      "\n",
      "SRC : ▁swedish▁automaker▁volvo▁'s▁wholly-owned▁subsidiary<unk>▁,▁which▁groups▁volvo▁'s▁activities▁not▁related▁to▁its▁core▁business▁sectors▁,▁will▁sell▁the▁swedish▁security▁and▁care▁company<unk>▁,▁volvo▁said▁wednesday▁.\n",
      "PRED : ▁volvo▁'s▁wholly-owned<unk>▁to▁sell<unk></s>\n",
      "\n",
      "SRC : ▁china▁'s▁communist▁party▁conceded▁thursday▁,▁the▁eve▁of▁its▁key▁##th▁congress▁,▁that▁it▁faced▁an▁<unk>▁uphill▁journey▁''▁against▁corruption▁but▁was▁determined▁to▁weed▁out▁endemic▁graft▁and▁bribery▁amongst▁the▁ruling▁elite▁.\n",
      "PRED : ▁china▁'s▁communists▁admit▁to▁uphill▁ride▁against▁corruption</s>\n",
      "\n",
      "SRC : ▁the▁dollar▁was▁steady▁to▁slightly▁softer▁in▁asian▁trade▁thursday▁,▁with▁the▁overall▁tone▁staying▁bearish▁after▁a▁drop▁in▁crude▁oil▁prices▁that▁should▁favor▁both▁the▁us▁and▁japanese▁economies▁,▁dealers▁said▁.\n",
      "PRED : ▁dollar▁steady▁to▁asian▁trade▁;▁dollar▁bearish▁after▁drop▁in▁oil▁prices</s>\n",
      "\n",
      "SRC : ▁chinese▁president▁jiang▁zemin▁has▁issued▁an▁ominous▁warning▁that▁stability▁will▁be▁maintained▁at▁any▁cost▁,▁defending▁the▁bloody▁suppression▁of▁the▁####▁pro-democracy▁movement▁just▁weeks▁before▁its▁fifth▁anniversary▁.\n",
      "PRED : ▁jiang▁warns▁against▁stability▁at▁all▁cost</s>\n",
      "\n",
      "SRC : ▁a▁northern▁nigerian▁secondary▁school▁student▁has▁been▁beaten▁to▁death▁by▁his▁mates▁who▁accused▁him▁of▁homosexual▁acts▁,▁a▁state-run▁radio▁reported▁saturday▁here▁.\n",
      "PRED : ▁nigerian▁school▁student▁beaten▁to▁death▁in▁mate▁beating</s>\n",
      "\n",
      "SRC : ▁us▁authorities▁arrested▁a▁man▁for▁allegedly▁conspiring▁to▁sell▁trade▁secrets▁from▁dow▁chemical▁company▁to▁companies▁in▁china▁,▁the▁justice▁department▁said▁wednesday▁.\n",
      "PRED : ▁us▁arrests▁man▁for▁alleged▁conspiracy▁to▁sell▁dow▁secrets▁to▁china</s>\n",
      "\n",
      "SRC : ▁chinese▁president▁jiang▁zemin▁friday▁opened▁the▁most▁important▁meeting▁of▁the▁ruling▁communist▁party▁in▁a▁decade▁by▁urging▁speedy▁economic▁change▁,▁while▁at▁the▁same▁time▁stressing▁strict▁political▁control▁.\n",
      "PRED : ▁jiang▁calls▁for▁economic▁change▁as▁china▁opens▁communist▁party▁meeting</s>\n",
      "\n",
      "SRC : ▁allegations▁of▁a▁kremlin▁scandal▁and▁cover-up▁during▁president▁boris▁yeltsin▁'s▁re-election▁campaign▁gained▁extra▁force▁friday▁with▁the▁publication▁of▁new▁material▁appearing▁to▁compromise▁his▁chief▁of▁staff▁anatoly▁chubais▁.\n",
      "PRED : ▁kremlin▁scandal▁claims▁gain▁extra▁force</s>\n",
      "\n",
      "SRC : ▁tutsi▁rebel▁chief▁laurent-desire▁kabila▁monday▁urged▁the▁international▁community▁to▁halt▁attacks▁on▁goma▁launched▁from▁the▁mugunga▁camp▁sheltering▁###,###▁refugees▁,▁or▁warned▁the▁rebels▁would▁act▁alone▁.\n",
      "PRED : ▁tutsi▁rebel▁chief▁urges▁international▁community▁to▁halt▁goma▁attacks</s>\n",
      "\n",
      "SRC : ▁ethiopian▁prime▁minister▁meles▁zenawi▁on▁thursday▁opened▁a▁summit▁in▁addis▁ababa▁of▁leaders▁from▁africa▁'s▁largest▁trading▁bloc▁,▁which▁will▁focus▁on▁developing▁their▁free▁trade▁area▁-lrb-▁fta▁-rrb-▁.\n",
      "PRED : ▁ethiopian▁pm▁opens▁summit▁on▁fta▁trade▁zone</s>\n",
      "\n",
      "SRC : ▁a▁man▁who▁killed▁french▁war▁criminal▁rene▁bousquet▁and▁called▁a▁press▁conference▁afterwards▁to▁boast▁about▁it▁has▁been▁sent▁for▁trial▁after<unk>▁said▁he▁was▁not▁mad▁,▁officials▁said▁thursday▁.\n",
      "PRED : ▁french▁war▁criminal▁who▁killed▁bousquet▁in▁court</s>\n",
      "\n",
      "SRC : ▁hong▁kong▁residents▁were▁warned▁on▁wednesday▁to▁avoid▁areas▁of▁heavy▁traffic▁as▁thick▁smog▁hung▁over▁the▁busiest▁business▁zones▁.\n",
      "PRED : ▁smog▁hung▁over▁busiest▁business▁zones▁warns▁hong▁kong▁residents</s>\n",
      "\n",
      "SRC : ▁zimbabwe▁'s▁opposition▁movement▁for▁democratic▁change▁-lrb-▁mdc▁-rrb-▁on▁sunday▁filed▁an▁application▁with▁the▁high▁court▁for▁an▁extension▁of▁this▁weekend▁'s▁presidential▁election▁,▁a▁lawyer▁for▁the▁party▁said▁.\n",
      "PRED : ▁zimbabwe▁opposition▁files▁case▁for▁extension▁of▁presidential▁election</s>\n",
      "\n",
      "SRC : ▁followers▁of▁norse▁mythology▁,▁who▁worship▁viking▁gods▁such▁as▁odin▁and▁thor▁,▁were▁on▁thursday▁officially▁recognized▁as▁a▁religious▁community▁in▁denmark▁,▁the▁ecclesiastical▁affairs▁ministry▁said▁.\n",
      "PRED : ▁spiritual▁community▁in▁denmark▁for▁norse▁spiritual▁believers</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0, len(valid_data.examples)), 20)\n",
    "for i in idxs:\n",
    "    src = vars(valid_data.examples[i])['src']\n",
    "    #trg = vars(valid_data.examples[i])['trg']\n",
    "    translation = translate_sentence2(src, model, device)\n",
    "\n",
    "    print(f\"SRC : {''.join(tokenizer.convert_ids_to_tokens(src))}\")\n",
    "    #print(f\"TRG : {''.join(tokenizer.convert_ids_to_tokens(trg))}\")\n",
    "    print(f\"PRED : {''.join(tokenizer.convert_ids_to_tokens(translation))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
