{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "from transformers import T5Model,T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁Hello', '▁world', '▁how', '▁are', '▁you', '?']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('Hello world how are you?')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8774, 296, 149, 33, 25, 58]\n"
     ]
    }
   ],
   "source": [
    "indexes = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> </s> <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "init_token = tokenizer.pad_token\n",
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 2\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes['t5-small']\n",
    "\n",
    "print(max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)\n",
    "\n",
    "TRG = Field(batch_first = True,\n",
    "          use_vocab = False,\n",
    "          tokenize = tokenize_and_cut,\n",
    "          preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "          init_token = init_token_idx,\n",
    "          eos_token = eos_token_idx,\n",
    "          pad_token = pad_token_idx,\n",
    "          unk_token = unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [('src', SRC), ('trg', TRG)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n",
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data = data.TabularDataset.splits(\n",
    "                path = '',\n",
    "                train = 'gigaword_train.csv',\n",
    "                format = 'csv',\n",
    "                fields = fields,\n",
    "                skip_header = True)\n",
    "\n",
    "train_data , valid_data = train_data[0].split(split_ratio=0.98,\n",
    "                                             random_state = random.seed(1234))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88811\n",
      "1812\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.examples))\n",
    "print(len(valid_data.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': [8, 8263, 8036, 89, 26, 1088, 16026, 1997, 1135, 12, 30242, 3, 15, 122, 63, 102, 17, 3, 31, 7, 4356, 661, 18, 1647, 7, 48, 471, 12, 4973, 789, 3, 2, 7712, 3, 31, 31, 16, 8, 166, 1751, 116, 8263, 2251, 4567, 12, 3, 11005, 524, 3, 9, 712, 3143, 3, 5], 'trg': [8263, 1088, 20143, 7, 12, 30242, 4356, 661, 18, 1647, 7]}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁the', '▁opposition', '▁wa', 'f', 'd', '▁party', '▁threatened', '▁sun', 'day', '▁to', '▁boycott', '▁', 'e', 'g', 'y', 'p', 't', '▁', \"'\", 's', '▁election', '▁run', '-', 'off', 's', '▁this', '▁week', '▁to', '▁protest', '▁government', '▁', '<unk>', '▁fraud', '▁', \"'\", \"'\", '▁in', '▁the', '▁first', '▁round', '▁when', '▁opposition', '▁parties', '▁failed', '▁to', '▁', 'clin', 'ch', '▁', 'a', '▁single', '▁seat', '▁', '.']\n",
      "['▁opposition', '▁party', '▁threaten', 's', '▁to', '▁boycott', '▁election', '▁run', '-', 'off', 's']\n"
     ]
    }
   ],
   "source": [
    "src_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[2000])['src'])\n",
    "trg_tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[2000])['trg'])\n",
    "\n",
    "print(src_tokens)\n",
    "print(trg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gauranga/anaconda3/envs/pt/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, valid_iterator = BucketIterator.splits(\n",
    "                                 (train_data, valid_data), \n",
    "                                 batch_size = BATCH_SIZE,\n",
    "                                 device = device,\n",
    "                                 sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.t5 = T5Model.from_pretrained('t5-small')\n",
    "        \n",
    "        self.out = nn.Linear(self.t5.config.to_dict()['d_model'],\n",
    "                             self.t5.config.to_dict()['vocab_size'])\n",
    "                \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        embedded = self.t5(input_ids=src,decoder_input_ids=trg) \n",
    "        \n",
    "        output = self.out(embedded[0])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = T5Network().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 76,988,544 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT ALL MODEL WEIGHTS AND BIASES TO HALF PRECISION\n",
    "# MODEL SIZE WILL REDUCE\n",
    "model = model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('t5_summ_model_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence2(sentence, eval_model, device, max_len = 50):\n",
    "    \n",
    "    eval_model.eval()\n",
    "\n",
    "    src_indexes = [init_token_idx] + sentence + [eos_token_idx]\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    trg_indexes = [init_token_idx]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = eval_model(src_tensor, trg_tensor)\n",
    "        \n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == eos_token_idx:\n",
    "            break\n",
    "\n",
    "    return trg_indexes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRC : ▁philippine▁president▁gloria▁arroyo▁on▁monday▁said▁she▁had▁reprimanded▁a▁senior▁adviser▁for▁announcing▁she▁would▁seek▁a▁second▁term▁in▁####▁.\n",
      "PRED : ▁philippine▁president▁reprimands▁senior▁adviser</s>\n",
      "\n",
      "SRC : ▁world▁champion▁michael▁schumacher▁on▁thursday▁warned▁that▁complacency▁could▁shatter▁his▁dream▁of▁a▁fifth▁world▁formula▁one▁crown▁.\n",
      "PRED : ▁schumacher▁warns▁of▁complacency▁in▁world▁title</s>\n",
      "\n",
      "SRC : ▁the▁dutch▁finance▁ministry▁announced▁on▁friday▁that▁it▁would▁float▁a▁six▁percent▁government▁bond▁issue▁for▁##▁years▁,▁the▁first▁one▁intended▁for▁covering▁the▁dutch▁government▁'s▁financing▁needs▁for▁####▁.\n",
      "PRED : ▁dutch▁finance▁ministry▁to▁float▁first▁bond▁issue▁for▁##▁years</s>\n",
      "\n",
      "SRC : ▁thirty-two▁journalists▁were▁killed▁in▁the▁commonwealth▁of▁independent▁states▁-lrb-▁cis▁-rrb-▁in▁####▁compared▁with▁##▁last▁year▁,▁a▁russian▁press▁freedom▁watchdog▁group▁told▁interfax▁news▁agency▁saturday▁.\n",
      "PRED : ▁##▁journalists▁killed▁in▁russian▁press▁freedom▁watchdog▁says</s>\n",
      "\n",
      "SRC : ▁thailand▁went▁to▁the▁polls▁sunday▁in▁what▁local▁media▁billed▁as▁probably▁the▁dirtiest▁campaign▁in▁decades▁,▁with▁rampant▁vote-buying▁and▁death▁threats▁against▁political▁canvassers▁and▁independent▁monitors▁.\n",
      "PRED : ▁thailand▁goes▁to▁the▁polls▁in▁a▁row</s>\n",
      "\n",
      "SRC : ▁nato▁leaders▁agreed▁thursday▁that▁they▁are▁ready▁to▁take▁<unk>▁effective▁action▁''▁to▁support▁un▁demands▁for▁iraq▁to▁disarm▁if▁needed▁,▁a▁diplomat▁said▁.\n",
      "PRED : ▁nato▁ready▁to▁take▁effective▁action▁to▁support▁un▁demands▁for▁iraq</s>\n",
      "\n",
      "SRC : ▁a▁town▁mayor▁on▁the▁philippine▁island▁of▁basilan▁on▁wednesday▁said▁his▁armed▁followers▁had▁abducted▁wives▁and▁relatives▁of▁known▁abu▁sayyaf▁rebels▁and▁offered▁to▁swap▁them▁for▁one▁of▁the▁hostages▁held▁by▁the▁guerrillas▁.\n",
      "PRED : ▁mayor▁says▁abu▁sayyaf▁abductors▁relatives▁of▁abu▁sayyaf</s>\n",
      "\n",
      "SRC : ▁philippine▁share▁prices▁closed▁#.#▁percent▁higher▁on▁friday▁following▁buying▁of▁key▁blue▁chips▁,▁analysts▁said▁.\n",
      "PRED : ▁philippine▁shares▁close▁#.#▁percent▁higher</s>\n",
      "\n",
      "SRC : ▁baton-wielding▁police▁arrested▁some▁#,###▁leftists▁here▁sunday▁after▁they▁tried▁to▁lay▁siege▁to▁a▁hotel▁housing▁##▁contestants▁of▁the▁miss▁world▁pageant▁.\n",
      "PRED : ▁police▁arrest▁#,###▁leftists▁in▁miss▁world▁siege</s>\n",
      "\n",
      "SRC : ▁us▁president▁bill▁clinton▁compared▁himself▁here▁wednesday▁with▁richard▁jewell▁,▁the▁former▁suspect▁in▁the▁deadly▁atlanta▁olympics▁bombing▁.\n",
      "PRED : ▁clinton▁compares▁himself▁with▁diamondl</s>\n",
      "\n",
      "SRC : ▁the▁european▁commission▁proposed▁wednesday▁to▁slap▁anti-dumping▁duties▁on▁chinese▁and▁vietnamese▁shoe▁imports▁,▁despite▁deep▁divisions▁among▁member▁states▁about▁the▁measures▁.\n",
      "PRED : ▁eu▁proposes▁to▁slash▁duties▁on▁chinese▁vietnamese▁shoes</s>\n",
      "\n",
      "SRC : ▁a▁tajik▁yak-##▁passenger▁airplane▁took▁off▁early▁friday▁from▁dushanbe▁for▁the▁afghan▁capital▁kabul▁friday▁,▁inaugurating▁a▁new▁commercial▁air▁link▁,▁the▁tajikistan▁airline▁said▁.\n",
      "PRED : ▁tajik▁plane▁opens▁new▁air▁link</s>\n",
      "\n",
      "SRC : ▁olympique▁marseille▁are▁to▁write▁to▁uefa▁to▁highlight▁what▁they▁regard▁as▁improper▁conduct▁by▁italian▁referee▁pierluigi▁collina▁before▁wednesday▁'s▁uefa▁cup▁final▁,▁in▁which▁spanish▁champions▁valencia▁beat▁the▁french▁side▁#-#▁.\n",
      "PRED : ▁marseille▁to▁write▁to▁uefa▁over▁collina</s>\n",
      "\n",
      "SRC : ▁indonesian▁share▁prices▁closed▁#.##▁percent▁lower▁monday▁,▁slipping▁against▁a▁backdrop▁of▁weaker▁regional▁markets▁,▁dealers▁said▁.\n",
      "PRED : ▁jakarta▁shares▁close▁#.##▁percent▁lower</s>\n",
      "\n",
      "SRC : ▁ireland▁votes▁wednesday▁in▁a▁referendum▁on▁whether▁to▁tighten▁the▁country▁'s▁already▁strict▁legislation▁on▁abortion▁.\n",
      "PRED : ▁ireland▁votes▁in▁referendum▁on▁abortion▁law</s>\n",
      "\n",
      "SRC : ▁the▁body▁of▁a▁soldier▁who▁was▁killed▁during▁a▁border▁patrol▁in▁papua▁province▁has▁been▁returned▁to▁indonesia▁from▁papua▁new▁guinea▁,▁the▁military▁said▁friday▁.\n",
      "PRED : ▁body▁of▁soldier▁killed▁in▁png▁returns▁to▁indonesia</s>\n",
      "\n",
      "SRC : ▁a▁##-year-old▁suspected▁serial▁killer▁who▁is▁believed▁to▁have▁murdered▁at▁least▁##▁women▁over▁the▁past▁few▁years▁has▁been▁arrested▁in▁southern▁russia▁,▁the▁itar-tass▁news▁agency▁reported▁.\n",
      "PRED : ▁suspected▁serial▁killer▁held▁in▁russia</s>\n",
      "\n",
      "SRC : ▁two▁passenger▁buses▁crashed▁head-on▁sending▁one▁into▁a▁gas▁pipeline▁which▁exploded▁,▁killing▁at▁least▁##▁people▁early▁wednesday▁in▁northeastern▁venezuela▁,▁officials▁said▁.\n",
      "PRED : ▁venezuela▁bus▁crash▁kills▁at▁least▁##</s>\n",
      "\n",
      "SRC : ▁irish▁prime▁minister▁bertie▁ahern▁began▁a▁two-day▁visit▁to▁malaysia▁sunday▁aimed▁at▁strengthening▁ties▁between▁the▁two▁countries▁,▁official▁media▁said▁.\n",
      "PRED : ▁irish▁pm▁begins▁malaysia▁visit</s>\n",
      "\n",
      "SRC : ▁a▁powerful▁blast▁was▁heard▁in▁the▁saudi▁capital▁riyadh▁toward▁midnight▁-lrb-▁####▁gmt▁-rrb-▁saturday▁,▁an▁afp▁correspondent▁reported▁.\n",
      "PRED : ▁powerful▁blast▁heard▁in▁saudi▁arabia</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = random.sample(range(0, len(valid_data.examples)), 20)\n",
    "for i in idxs:\n",
    "    src = vars(valid_data.examples[i])['src']\n",
    "    #trg = vars(valid_data.examples[i])['trg']\n",
    "    translation = translate_sentence2(src, model, device)\n",
    "\n",
    "    print(f\"SRC : {''.join(tokenizer.convert_ids_to_tokens(src))}\")\n",
    "    #print(f\"TRG : {''.join(tokenizer.convert_ids_to_tokens(trg))}\")\n",
    "    print(f\"PRED : {''.join(tokenizer.convert_ids_to_tokens(translation))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_summary(txt):\n",
    "    txt_tokens = tokenizer.tokenize(txt)\n",
    "    txt_ids = tokenizer.convert_tokens_to_ids(txt_tokens)\n",
    "    pred = translate_sentence2(txt_ids, model, device)\n",
    "    pred_tokens = tokenizer.convert_ids_to_tokens(pred)\n",
    "    \n",
    "    return ''.join(pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"the united nations confirmed tuesday that un secretary general kofi annan will hold a third round of talks with iraqi foreign minister naji sabri in vienna in early july .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁un▁confirms▁annan▁to▁hold▁third▁round▁of▁talks▁with▁iraqi▁fm</s>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_summary(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
